# Importing Required Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import numpy as np
import pickle
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import (
    mean_squared_error, accuracy_score,
    ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score
)
from sklearn.inspection import permutation_importance

warnings.filterwarnings('ignore')

# Load and Clean the Data
players_df = pd.read_csv('./datasets/players.csv')
team_df = pd.read_csv('./datasets/team.csv')

# Automatically handle 'Unnamed' columns
players_df.rename(columns=lambda x: x if not x.startswith('Unnamed') else f'Col_{x.split(":")[-1]}', inplace=True)
team_df.rename(columns=lambda x: x if not x.startswith('Unnamed') else f'Col_{x.split(":")[-1]}', inplace=True)

# Remove columns with all missing values
players_cleaned = players_df.dropna(axis=1, how='all')
team_cleaned = team_df.dropna(axis=1, how='all')

# Check and handle duplicates
players_cleaned.drop_duplicates(inplace=True)
team_cleaned.drop_duplicates(inplace=True)

# Descriptive Statistics
print("Players Dataset Statistics:")
print(players_cleaned.describe())

print("\nTeams Dataset Statistics:")
print(team_cleaned.describe())

# Visualization: Points vs. Minutes Played (From players_cleaned)
plt.figure(figsize=(10, 6))
sns.scatterplot(data=players_cleaned, x='Min', y='PTS', hue='Team', palette='tab10')
plt.title('Player Points vs. Minutes Played')
plt.xlabel('Minutes Played')
plt.ylabel('Points')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Add Efficiency and Player Efficiency Rating (PER)
players_cleaned['Efficiency'] = players_cleaned['PTS'] / players_cleaned['Min']
players_cleaned['Efficiency'] = players_cleaned['Efficiency'].fillna(0)

# PER calculation 
players_cleaned['PER'] = (
                                 players_cleaned['PTS'] + players_cleaned['REB'] +
                                 players_cleaned['AST'] + players_cleaned['STL'] + players_cleaned['BLK']
                         ) - players_cleaned['TOV']

# Feature Engineering: Team-Level Efficiency and Performance Classification

# Assuming 'PTS', 'REB', 'AST', 'STL', 'BLK', and 'TOV' are available in team_cleaned
# Calculate team-level efficiency based on available stats. If needed, you can aggregate per team.

# Example: Calculate basic team efficiency by averaging player-level stats (here using placeholder column names)
# Adjust these lines to match your dataset structure.

team_cleaned['Efficiency'] = (
                                     team_cleaned['PTS'] + team_cleaned['REB'] + team_cleaned['AST'] +
                                     team_cleaned['STL'] + team_cleaned['BLK'] - team_cleaned['TOV']
                             ) / team_cleaned.shape[0]  # This assumes there are multiple rows for each team

# Classify teams based on their efficiency
team_cleaned['Performance'] = pd.cut(
    team_cleaned['Efficiency'],
    bins=[-np.inf, 0.5, 1.0, np.inf],  # Example bins for classification
    labels=['Bad', 'Average', 'Good']
)

# Visualization: Team Performance Categories (Violin Plot)
plt.figure(figsize=(10, 6))
sns.violinplot(data=team_cleaned, x='Performance', y='Efficiency', palette='muted')
plt.title('Efficiency Distribution by Team Performance Category')
plt.show()

# Model Training Data Preparation for Regression (from players_cleaned)
X_reg = players_cleaned[['Min', 'REB', 'AST', 'STL', 'BLK', 'TOV']].fillna(0)
y_reg = players_cleaned['Efficiency']

X_class = X_reg
y_class = players_cleaned['Performance']

# Split data for regression and classification
X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)
X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.2,
                                                                            random_state=42)
# Regression: Ridge Model with Cross-Validation
ridge = Ridge(alpha=1.0)
ridge_cv = cross_val_score(ridge, X_reg, y_reg, cv=KFold(n_splits=5))
ridge.fit(X_reg_train, y_reg_train)
y_reg_pred = ridge.predict(X_reg_test)

# Permutation Feature Importance for Regression
perm_importance_reg = permutation_importance(ridge, X_reg_test, y_reg_test, n_repeats=10, random_state=42)
importance_df = pd.DataFrame({
    'Feature': X_reg.columns,
    'Importance': perm_importance_reg.importances_mean
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance (Permutation) - Regression Model')
plt.show()

# Determine Threshold and Drop Unimportant Features (Regression)
importance_threshold = 0.01  # Example threshold for feature importance

# Filter regression features
important_features_reg = importance_df[importance_df['Importance'] > importance_threshold]['Feature'].tolist()
X_reg_filtered = X_reg[important_features_reg]

# Re-split data with filtered features
X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg_filtered, y_reg, test_size=0.2,
                                                                    random_state=42)

# Regression Evaluation
mse = mean_squared_error(y_reg_test, y_reg_pred)
rmse = np.sqrt(mse)
print("Regression Model Evaluation:")
print(f"Cross-Validation Score: {ridge_cv.mean():.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")

# Save Ridge Model
with open('./models/ridge_model.pkl', 'wb') as file:
    pickle.dump(ridge, file)

# Classification: Logistic Regression with Hyperparameter Tuning
param_grid = {'C': [0.1, 1, 10, 100], 'solver': ['lbfgs', 'liblinear']}
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=KFold(n_splits=5), scoring='accuracy')
grid.fit(X_class_train, y_class_train)

classifier = grid.best_estimator_
y_class_pred = classifier.predict(X_class_test)
y_class_proba = classifier.predict_proba(X_class_test)

# Permutation Feature Importance for Classification
perm_importance_class = permutation_importance(classifier, X_class_test, y_class_test, n_repeats=10, random_state=42)
class_importance_df = pd.DataFrame({
    'Feature': X_class.columns,
    'Importance': perm_importance_class.importances_mean
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=class_importance_df)
plt.title('Feature Importance (Permutation) - Classification Model')
plt.show()

# Determine Threshold and Drop Unimportant Features (Classification)
important_features_class = class_importance_df[class_importance_df['Importance'] > importance_threshold][
    'Feature'].tolist()
X_class_filtered = X_class[important_features_class]

# Re-split data with filtered features
X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class_filtered, y_class, test_size=0.2,
                                                                            random_state=42)

# For one-hot encoded arrays
if len(y_class_test.shape) > 1:
    y_class_test = np.argmax(y_class_test, axis=1)

if len(y_class_pred.shape) > 1:
    y_class_pred = np.argmax(y_class_pred, axis=1)

# Classification Evaluation
accuracy = accuracy_score(y_class_test, y_class_pred)
precision = precision_score(y_class_test, y_class_pred, average='weighted')
recall = recall_score(y_class_test, y_class_pred, average='weighted')
f1 = f1_score(y_class_test, y_class_pred, average='weighted')

# For roc_auc, ensure y_class_test is in the correct format (either class labels or one-hot encoded)
roc_auc = roc_auc_score(y_class_test, y_class_proba, multi_class='ovr')

# Print the evaluation metrics
print("\nClassification Model Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Save Logistic Model
with open('./models/logistic_model.pkl', 'wb') as file:
    pickle.dump(classifier, file)

# Visualization: Violin Plot for Performance Categories (from team_cleaned)
plt.figure(figsize=(10, 6))
sns.violinplot(data=team_cleaned, x='Performance', y='Efficiency', palette='muted')
plt.title('Efficiency Distribution by Team Performance Category')
plt.show()
